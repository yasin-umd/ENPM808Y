{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "The goal of this assignment is to understand and implement different types of regression and classification algorithms using Python. The specific algorithms to be used are:\n",
    "\n",
    "### Algorithms: \n",
    "* Regression \n",
    "* Logistic Regression \n",
    "* Naive Bayes\n",
    "\n",
    "### Tasks: \n",
    "1. Perform EDA on the dataset and pre-processing if necessary.\n",
    "2. Train a linear regression , logistic regression and naive bayes model using the dataset.\n",
    "3. Evaluate the model's performance using appropriate metrics (e.g. accuracy, precision, recall, etc.).\n",
    "4. Use the trained model to make predictions on unseen data.\n",
    "5. Compare the perfomance of Linear Regression, Logistic Regression and Naive Bayes.\n",
    "6. Analyze the results and draw conclusions.\n",
    "\n",
    "\n",
    "### DATASET - *AI4I 2020 Predictive Maintenance*\n",
    "\n",
    "#### Description\n",
    "\n",
    "The AI4I 2020 Predictive Maintenance Dataset is a synthetic dataset that reflects real predictive maintenance data encountered in industry. Since real predictive maintenance datasets are generally difficult to obtain and in particular difficult to publish, we present and provide a synthetic dataset that reflects real predictive maintenance encountered in industry to the best of our knowledge.\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "The dataset consists of 10000 data points stored as rows with 14 features in columns\n",
    "\n",
    "- UID: unique identifier ranging from 1 to 10000\n",
    "- product ID: consisting of a letter L, M, or H for low (50% of all products), medium (30%) and high (20%) as product quality variants and a variant-specific serial number\n",
    "- air temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\n",
    "- process temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\n",
    "- rotational speed [rpm]: calculated from a power of 2860 W, overlaid with a normally distributed noise\n",
    "- torque [Nm]: torque values are normally distributed around 40 Nm with a f = 10 Nm and no negative values.\n",
    "- tool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.\n",
    "- 'machine failure' label that indicates, whether the machine has failed in this particular datapoint for any of the following failure modes are true.\n",
    "\n",
    "The machine failure consists of five independent failure modes :\n",
    "\n",
    "- tool wear failure (TWF): the tool will be replaced of fail at a randomly selected tool wear time between 200-240 mins (120 times in our dataset). At this point in time, the tool is replaced 69 times, and fails 51 times (randomly assigned).\n",
    "\n",
    "- heat dissipation failure (HDF): heat dissipation causes a process failure, if the difference between air- and process temperature is below 8.6 K and the tools rotational speed is below 1380 rpm. This is the case for 115 data points.\n",
    "- power failure (PWF): the product of torque and rotational speed (in rad/s) equals the power required for the process. If this power is below 3500 W or above 9000 W, the process fails, which is the case 95 times in our dataset.\n",
    "- overstrain failure (OSF): if the product of tool wear and torque exceeds 11,000 minNm for the L product variant (12,000 M, 13,000 H), the process fails due to overstrain. This is true for 98 datapoints.\n",
    "- random failures (RNF): each process has a chance of 0,1 % to fail regardless of its process parameters. This is the case for only 5 datapoints, less than could be expected for 10,000 datapoints in our dataset.\n",
    "\n",
    "If at least one of the above failure modes is true, the process fails and the 'machine failure' label is set to 1. It is therefore not transparent to the machine learning method, which of the failure modes has caused the process to fail\n",
    "\n",
    "\n",
    "# Inputs \n",
    "- UID\n",
    "- Product ID\n",
    "- Air temperature [K]\n",
    "- Process temperature [K]\n",
    "- Rotational speed [rpm]\n",
    "- Torque [Nm]\n",
    "- Tool wear [min]\n",
    "\n",
    "# Output\n",
    "- Machine failure (1 = failed, 0 = not failed)\n",
    "\n",
    "\n",
    "Dataset Link: https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset\n",
    "\n",
    "### Submission\n",
    "\n",
    "Kindly play around with the exploratory data analysis and model building. You can also try to improve the model's performance by tuning the hyperparameters.\n",
    "\n",
    "Submit your assignment as a Jupyter notebook on your GitHub repository. The notebook should contain all the code and outputs. You can also submit a PDF version of the notebook. \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the data from the file 'ai4i2020.csv' into a Pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ai4i2020.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "- The dataset has 10000 rows and 14 columns and we perform EDA on the dataset to understand the data better by checking for missing values, outliers, and other anomalies in the data.There is no missing value or duplicate value in the dataset.\n",
    "\n",
    "- The dataset has 7 numerical columns and 7 categorical columns. The numerical columns are 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Machine failure', 'UID'. The categorical columns are 'Product ID', 'Machine failure', 'Tool wear failure', 'Heat dissipation failure', 'Power failure', 'Overstrain failure', 'Random failure'.\n",
    "\n",
    "- We check for outliers in the numerical columns using boxplots. We find that there are outliers in the 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]' columns. We remove the outliers using the IQR method.\n",
    "\n",
    "- Convert the categorical columns to numerical columns using label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate values and drop them\n",
    "print(\"Checking for duplicate values: \", df.duplicated().sum() != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the numeric columns data type to float\n",
    "\n",
    "df['Process temperature [K]'] = df['Process temperature [K]'].astype(float)\n",
    "df['Rotational speed [rpm]'] = df['Rotational speed [rpm]'].astype(float)\n",
    "df['Torque [Nm]'] = df['Torque [Nm]'].astype(float)\n",
    "df['Tool wear [min]'] = df['Tool wear [min]'].astype(float)\n",
    "df['Machine failure'] = df['Machine failure'].astype(float)\n",
    "\n",
    "\n",
    "ds = df[\"Type\"].value_counts().reset_index()[:28]\n",
    "ds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first character and set to numeric dtype\n",
    "df['Product ID'] = df['Product ID'].apply(lambda x: x[1:])\n",
    "df['Product ID'] = pd.to_numeric(df['Product ID'])\n",
    "\n",
    "\n",
    "# Convert the Type column M,L,H to 0,1,2\n",
    "df['Type'] = df['Type'].apply(lambda x: 0 if x == 'L' else 1 if x == 'M' else 2)\n",
    "\n",
    "\n",
    "\n",
    "# Show both plots in the same figure using subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].pie(df['Type'].value_counts(), labels=df['Type'].value_counts().index, autopct='%1.1f%%')\n",
    "ax[0].set_title('Pie chart of Type')\n",
    "sns.histplot(data=df, x='Product ID', hue='Type', ax=ax[1])\n",
    "ax[1].set_title('Histogram of Product ID')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the UDI and Product ID column as it is not needed for the model training\n",
    "df_ = df.copy()\n",
    "df = df_.drop(['UDI', 'Product ID'], axis=1)\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Print count of Unique values of each column\n",
    "for col in df.columns:\n",
    "    print(col, df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix \n",
    "\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualizations of the data and their correlations with the target variable\n",
    "\n",
    "numeric_cols = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]','Type']\n",
    "sns.pairplot(df.loc[:,numeric_cols],hue=\"Type\",diag_kind='kde',kind='scatter')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- We now analyse the correlation between the features and the target variable 'Machine failure'. We find that the 'Tool wear [min]' column has the highest correlation with the target variable. We also find that the 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]' columns have a high correlation with the target variable.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = df.drop('Type', axis=1)\n",
    "y = df['Type']\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "    \n",
    "\n",
    " # Print the shapes of the training and testing sets and top 5 rows of the training set , X_train, y_train and X_test, y_test\n",
    "print(\"----Training Data ----\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"----Test Data ----\")\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Training Set:\", X_train[i], y_train.iloc[i])\n",
    "    print(\"Testing Set:\", X_test[i], y_test.iloc[i])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "- Linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\n",
    "\n",
    "- The mathematical equation for linear regression is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "- The goal of linear regression is to find the best fit line for the data. The best fit line is the line that has the least error. The error is the difference between the actual value and the predicted value. The error is calculated using the mean squared error function.\n",
    "\n",
    "- The mean squared error function is:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$  \n",
    "\n",
    "\n",
    "### Logistic Regression \n",
    "\n",
    "- Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n",
    "\n",
    "- The mathematical equation for logistic regression is:\n",
    "\n",
    "$$\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "- The goal of logistic regression is to find the best fit line for the data. The best fit line is the line that has the least error. The error is the difference between the actual value and the predicted value. The error is calculated using the mean squared error function.\n",
    "\n",
    "- The mean squared error function is:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "- Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "\n",
    "- The mathematical equation for naive bayes is:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "- The goal of naive bayes is to find the best fit line for the data. The best fit line is the line that has the least error. The error is the difference between the actual value and the predicted value. The error is calculated using the mean squared error function.\n",
    "\n",
    "- The mean squared error function is:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "# TODO: Train a linear regression model and print the model's coefficients and intercept use lr as the model name \n",
    "\n",
    "# lr : Linear Regression model  , y_pred : Predicted values of y_test  \n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "lr_mse = mean_squared_error(y_test, y_pred)\n",
    "lr_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Store the results in a dictionary\n",
    "results = {'Linear Regression': [lr_mse, lr_r2]}\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "\n",
    "# TODO: Train a logistic regression model \n",
    "\n",
    "# lr : Logistic Regression model  , y_pred_lr : Predicted values of y_test \n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "log_train = round(lr.score(X_train, y_train) * 100, 2)\n",
    "log_accuracy = round(accuracy_score(y_pred_lr, y_test) * 100, 2)\n",
    "\n",
    "\n",
    "print(\"Training Accuracy    :\",log_train ,\"%\")\n",
    "print(\"Model Accuracy Score :\",log_accuracy ,\"%\")\n",
    "print(\"\\033[1m--------------------------------------------------------\\033[0m\")\n",
    "print(\"Classification_Report: \\n\",classification_report(y_test,y_pred_lr,zero_division=0))\n",
    "print(\"\\033[1m--------------------------------------------------------\\033[0m\")\n",
    "plot_confusion_matrix(lr, X_test, y_test)\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a Naive Bayes Classifier\n",
    "\n",
    "\"TODO: Train a Naive Bayes model and print the model's coefficients and intercept use lr as the model name \"\n",
    "\n",
    "\" gnb : Naive Bayes Model  , y_pred_gnb : Predicted values of y_test \"\n",
    "\n",
    "\n",
    "gnb_train = round(gnb.score(X_train, y_train) * 100, 2)\n",
    "\n",
    "gnb_accuracy = round(accuracy_score(y_pred_gnb, y_test) * 100, 2)\n",
    "\n",
    "print(\"Training Accuracy    :\",gnb_train ,\"%\")\n",
    "print(\"Model Accuracy Score :\",gnb_accuracy ,\"%\")\n",
    "\n",
    "print(\"\\033[1m--------------------------------------------------------\\033[0m\")\n",
    "\n",
    "print(\"Classification_Report: \\n\",classification_report(y_test,y_pred_gnb))\n",
    "\n",
    "print(\"\\033[1m--------------------------------------------------------\\033[0m\")\n",
    "\n",
    "plot_confusion_matrix(gnb, X_test, y_test)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_gnb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results of the models\n",
    "\n",
    "results['Logistic Regression'] = [log_accuracy, log_train]\n",
    "results['Naive Bayes'] = [gnb_accuracy, gnb_train]\n",
    "results_df = pd.DataFrame(results, index=['Accuracy', 'Training Accuracy']).T\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a40522a8eb7aacb1bd29b236cac1d994c6cb7aaf249904f7446f2c7da9d8006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}